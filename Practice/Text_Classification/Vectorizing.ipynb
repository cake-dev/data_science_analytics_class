{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example: 4 documents\n",
    "X_train = [\n",
    "    'call you tonight',\n",
    "    'call me a cab',\n",
    "    'please call me... PLEASE',\n",
    "    'he called the police'\n",
    "]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using countvectorizer to convert text into a matrix of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer() # with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"learn the vocabulary\"\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exmaine the fitted vocabulary\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training data into a 'document-term matrix'\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names_out(), index=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ['please don\\'t call me']\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "pd.DataFrame(X_test_dtm.toarray(), columns=vect.get_feature_names_out(), index=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stop_words:** Stop words are words like [I, a, an, this, the, ...] that don't add much meaning to a sentence. We can remove them to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "vect.fit(X_train)\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of scikit learn stop words\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "\n",
    "sorted(list(_stop_words.ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ngram_range**: An n-gram is a sequence of n words. For example, \"apple juice\" is a 2-gram (aka a bigram), and \"I love apple juice\" is a 4-gram (aka a four-gram). The ngram_range parameter lets us specify the range of n-gram sizes we want to include in our features. In the example above, we included unigrams (ngram_range=(1,1)) and bigrams (ngram_range=(2,2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 3)) # 1 grams, 2 grams, 3 grams\n",
    "vect.fit(X_train)\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_df / min_df:** When building the vocabulary, we can set the maximum document frequency (max_df) and minimum document frequency (min_df). If the word frequency is below min_df OR above max_df, the word is ignored. This allows us to exclude words that are too rare or too common to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore items that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)\n",
    "vect.fit(X_train)\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only show items that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df=2)\n",
    "vect.fit(X_train)\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes classifiers are a collection of classification algorithms based on Bayes' Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.\n",
    "\n",
    "\"Naive\" assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, words in a sentence are independent of each other.\n",
    "\n",
    "It works on the principle of conditional probability. Conditional probability is the probability of something happening, given that something else has already occurred.\n",
    "\n",
    "Bayes Theorem: \n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Pros:\n",
    "- Very fast\n",
    "- Simple to implement\n",
    "\n",
    "\n",
    "<u>Example</u>\n",
    "\n",
    "Training documents:\n",
    "\n",
    "| Column | Document | Label |\n",
    "| :-- | --: |  --: |\n",
    "| Doc1 | \"basketball is a great game\" | not politics |\n",
    "| Doc2 | \"the election is over\" | politics |\n",
    "| Doc3 | \"very clean debate\" | politics |\n",
    "| Doc4 | \"a close but forgettable race\" | not politics |\n",
    "| Doc5 | \"the election is a race\" | politics |\n",
    "\n",
    "Vocabulary: basketball, great, ... , race (ignore stop words)\n",
    "new document: \"a very close race\"\n",
    "\n",
    "<u>goal</u>: predict the label of the new document i.e. to estimate p(politics | \"a very close race\") or p(not politics | \"a very close race\")\n",
    "\n",
    "Naive bayes predicts the label with the Largest Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(politics | \"a .. race\") = \\frac{1}{p(\"a .. race\")}*p(\"a .. race\" | politics)*p(politics)$ the end is the prior probability, the probability of the label\n",
    "\n",
    "$P(not-pol | \"a .. race\") = \\frac{1}{p(\"a .. race\")}*p(\"a .. race\" | not-pol)*p(not pol)$ before the prior is the likelihood\n",
    "\n",
    "$P(politics) = \\frac{number\\ of\\ politics\\ docs}{total\\ number\\ of\\ docs} = \\frac{3}{5}$\n",
    "\n",
    "$P(not-politics) = \\frac{2}{5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes:\n",
    "\n",
    "$P(\"a...race\" | politics) = P(\"very\" | politics) * P(\"close\" | politics) * P(\"race\" | politics)$\n",
    "\n",
    "$P(\"very\" | politics) = number\\ of\\ times\\ \"very\"\\ appears\\ in\\ politics\\ docs / total\\ number\\ of\\ words\\ in\\ politics\\ docs = 1+\\alpha/7+11\\alpha$\n",
    "\n",
    "$P(\"close\" | politics) = 0+\\alpha/7+11\\alpha$\n",
    "\n",
    "$P(\"race\" | politics) = 1+\\alpha/7+11\\alpha$\n",
    "\n",
    "We dont have \"close\" in politics docs, so we use Laplace smoothing using an alpha parameter (from 0 to 1) to avoid 0 probability\n",
    "\n",
    "We can add the alpha parameter to the numerator and the number of words in the vocabulary times alpha to the denominator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sportsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example: 4 documents\n",
    "X_train = [\n",
    "    'call you tonight',\n",
    "    'call me a cab',\n",
    "    'please call me... PLEASE',\n",
    "    'he called the police'\n",
    "]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using countvectorizer to convert text into a matrix of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer() # with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"learn the vocabulary\"\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exmaine the fitted vocabulary\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting training data into a 'document-term matrix'\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names_out(), index=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ['please don\\'t call me']\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "pd.DataFrame(X_test_dtm.toarray(), columns=vect.get_feature_names_out(), index=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stop_words:** Stop words are words like [I, a, an, this, the, ...] that don't add much meaning to a sentence. We can remove them to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "vect.fit(X_train)\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of scikit learn stop words\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "\n",
    "sorted(list(_stop_words.ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ngram_range**: An n-gram is a sequence of n words. For example, \"apple juice\" is a 2-gram (aka a bigram), and \"I love apple juice\" is a 4-gram (aka a four-gram). The ngram_range parameter lets us specify the range of n-gram sizes we want to include in our features. In the example above, we included unigrams (ngram_range=(1,1)) and bigrams (ngram_range=(2,2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cab', 'call', 'call me', 'call me cab', 'call me please',\n",
       "       'call you', 'call you tonight', 'called', 'called the',\n",
       "       'called the police', 'he', 'he called', 'he called the', 'me',\n",
       "       'me cab', 'me please', 'please', 'please call', 'please call me',\n",
       "       'police', 'the', 'the police', 'tonight', 'you', 'you tonight'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 3)) # 1 grams, 2 grams, 3 grams\n",
    "vect.fit(X_train)\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_df / min_df:** When building the vocabulary, we can set the maximum document frequency (max_df) and minimum document frequency (min_df). If the word frequency is below min_df OR above max_df, the word is ignored. This allows us to exclude words that are too rare or too common to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore items that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)\n",
    "vect.fit(X_train)\n",
    "vect.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sportsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

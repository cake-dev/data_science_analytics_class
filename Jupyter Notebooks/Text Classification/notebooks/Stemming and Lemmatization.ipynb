{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\javier.perez-\n",
      "[nltk_data]     alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\javier.perez-\n",
      "[nltk_data]     alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\javier.perez-\n",
      "[nltk_data]     alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\javier.perez-\n",
      "[nltk_data]     alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import NLTK (natural language toolkit)\n",
    "import nltk \n",
    "nltk.download('wordnet') # \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4') # open multilingual wordnet library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are techniques to nomalize text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading -> read\n",
    "\n",
    "Books -> book\n",
    "\n",
    "Stories -> stori (from stemming) or story (from lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem with the bag-of-words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy training data\n",
    "X_train = ['I love the book','This is a great book','The fit is great','I love the shoes']\n",
    "y_train = ['books','books','clothings','clothings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>fit</th>\n",
       "      <th>great</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>shoes</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love the book</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This is a great book</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The fit is great</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I love the shoes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      book  fit  great  is  love  shoes  the  this\n",
       "I love the book          1    0      0   0     1      0    1     0\n",
       "This is a great book     1    0      1   1     0      0    0     1\n",
       "The fit is great         0    1      1   1     0      0    1     0\n",
       "I love the shoes         0    0      0   0     1      1    1     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "pd.DataFrame(data=X_train_dtm.toarray(),columns=vect.get_feature_names_out(),index=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a naive bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['books', 'clothings', 'clothings', 'books'], dtype='<U9')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy testing data \n",
    "X_test = ['I like the book','Shoes are alright','I love the books','I lost a shoe']\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "nb_clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions for 'I love the books' and 'I lost a shoe' are wrong. Why? Because the model hasn't seen the words 'books' and 'shoe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organ'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# organize, organizes, and organizing\n",
    "stemmer.stem('organize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('organizes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('organizing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer breaks a sentence into its individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'the', 'books', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'I love the books.'\n",
    "words = word_tokenize(phrase)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'the', 'book', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love the book .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatizer expects the parts of speech; by default, each token is a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('eats', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('ate', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('love', 'VBP'), ('the', 'DT'), ('books', 'NNS'), ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parts of speech tagging\n",
    "pos_list = nltk.pos_tag(words)\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process parts of speech function\n",
    "def process_pos(pos):\n",
    "    if pos.startswith('J'): # adjectives\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'): # verbes\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'): # nouns\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'): # adverbs\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'the', 'book', '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=process_pos(pos)) for word,pos in nltk.pos_tag(words)]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love the book .'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of most common words in english: this, that, he, it, ... They don't add much meaning to the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here is an example sentence demostrating the removal of stopwords'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'here is an example sentence demostrating the removal of stopwords'\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example sentence demostrating removal stopwords'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(phrase)\n",
    "stripped_phrase = [word for word in words if word not in stop_words]\n",
    "\" \".join(stripped_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuation = [punc for punc in string.punctuation]\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello How are you'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Hello! How are you?'\n",
    "words = word_tokenize(phrase)\n",
    "stripped_phrase = [word for word in words if word not in punctuation]\n",
    "\" \".join(stripped_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  My wife took me here on my birthday for breakf...      5\n",
       "1  I have no idea why some people give bad review...      5\n",
       "2  love the gyro plate. Rice is so good and I als...      4\n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
       "4  General Manager Scott Petello is a good egg!!!...      5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/um-perez-alvaro/Data-Science-Practice/master/Data/yelp.csv'\n",
    "yelp = pd.read_csv(url)[['text','stars']]\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1       5\n",
       "2       4\n",
       "3       5\n",
       "4       5\n",
       "       ..\n",
       "9995    3\n",
       "9996    4\n",
       "9997    4\n",
       "9998    2\n",
       "9999    5\n",
       "Name: stars, Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>Yes I do rock the hipster joints.  I dig this ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>Only 4 stars? \\n\\n(A few notes: The folks that...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>I'm not normally one to jump at reviewing a ch...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>Let's see...what is there NOT to like about Su...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4086 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars\n",
       "0     My wife took me here on my birthday for breakf...      5\n",
       "1     I have no idea why some people give bad review...      5\n",
       "2     Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
       "3     General Manager Scott Petello is a good egg!!!...      5\n",
       "4     Drop what you're doing and drive here. After I...      5\n",
       "...                                                 ...    ...\n",
       "4081  Yes I do rock the hipster joints.  I dig this ...      5\n",
       "4082  Only 4 stars? \\n\\n(A few notes: The folks that...      5\n",
       "4083  I'm not normally one to jump at reviewing a ch...      5\n",
       "4084  Let's see...what is there NOT to like about Su...      5\n",
       "4085  4-5 locations.. all 4.5 star average.. I think...      5\n",
       "\n",
       "[4086 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews that only contains the 5-stars and 1-star reviews\n",
    "yelp = yelp[yelp.stars.isin([1,5])].reset_index(drop=True)\n",
    "yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print(yelp.loc[0,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wife take birthday breakfast excellent weather perfect make sit outside overlook ground absolute pleasure waitress excellent food arrive quickly semi-busy saturday morning look like place fill pretty quickly early get good favor get bloody mary phenomenal simply best 've ever 'm pretty sure use ingredient garden blend fresh order amaze everything menu look excellent white truffle scramble egg vegetable skillet tasty delicious come 2 piece griddle bread amaze absolutely make meal complete best `` toast '' 've ever anyway ca n't wait go back\n"
     ]
    }
   ],
   "source": [
    "text = yelp.loc[0,'text']\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=process_pos(pos)) \n",
    "                    for word,pos in nltk.pos_tag(words) \n",
    "                    if word not in stop_words and word not in punctuation]\n",
    "print(' '.join(lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(yelp)):\n",
    "    text = yelp.loc[i,'text']\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos=process_pos(pos)) \n",
    "                        for word,pos in nltk.pos_tag(words) \n",
    "                        if word not in stop_words and word not in punctuation]\n",
    "    yelp.loc[i,'processed_text'] = ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "      <td>wife take birthday breakfast excellent weather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "      <td>idea people give bad review place go show plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "      <td>rosie dakota love chaparral dog park 's conven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "      <td>general manager scott petello good egg go deta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>5</td>\n",
       "      <td>drop 're drive eat go back next day food good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>Yes I do rock the hipster joints.  I dig this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>yes rock hipster joint dig place little bit sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>Only 4 stars? \\n\\n(A few notes: The folks that...</td>\n",
       "      <td>5</td>\n",
       "      <td>4 star note folk rat place low must isolate in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>I'm not normally one to jump at reviewing a ch...</td>\n",
       "      <td>5</td>\n",
       "      <td>'m normally one jump review chain restaurant e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>Let's see...what is there NOT to like about Su...</td>\n",
       "      <td>5</td>\n",
       "      <td>let 's see ... like surprise stadium well 9.50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>5</td>\n",
       "      <td>4-5 location .. 4.5 star average .. think ariz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4086 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars  \\\n",
       "0     My wife took me here on my birthday for breakf...      5   \n",
       "1     I have no idea why some people give bad review...      5   \n",
       "2     Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5   \n",
       "3     General Manager Scott Petello is a good egg!!!...      5   \n",
       "4     Drop what you're doing and drive here. After I...      5   \n",
       "...                                                 ...    ...   \n",
       "4081  Yes I do rock the hipster joints.  I dig this ...      5   \n",
       "4082  Only 4 stars? \\n\\n(A few notes: The folks that...      5   \n",
       "4083  I'm not normally one to jump at reviewing a ch...      5   \n",
       "4084  Let's see...what is there NOT to like about Su...      5   \n",
       "4085  4-5 locations.. all 4.5 star average.. I think...      5   \n",
       "\n",
       "                                         processed_text  \n",
       "0     wife take birthday breakfast excellent weather...  \n",
       "1     idea people give bad review place go show plea...  \n",
       "2     rosie dakota love chaparral dog park 's conven...  \n",
       "3     general manager scott petello good egg go deta...  \n",
       "4     drop 're drive eat go back next day food good ...  \n",
       "...                                                 ...  \n",
       "4081  yes rock hipster joint dig place little bit sc...  \n",
       "4082  4 star note folk rat place low must isolate in...  \n",
       "4083  'm normally one jump review chain restaurant e...  \n",
       "4084  let 's see ... like surprise stadium well 9.50...  \n",
       "4085  4-5 location .. 4.5 star average .. think ariz...  \n",
       "\n",
       "[4086 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = yelp.stars\n",
    "X = yelp.processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('vect', CountVectorizer(max_features=5000,ngram_range=(1,2))), \n",
    "    ('clf', MultinomialNB()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[140,  27],\n",
       "       [ 48, 807]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266144814090019"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the model choose between 5-stars or 1-star ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the vocabulary of X_train\n",
    "words = pipe['vect'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['clf'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times each word appears across all 1-star docs\n",
    "bad_word_count = pipe['clf'].feature_count_[0,:]\n",
    "# number of times each word appears across all 5-stars docs\n",
    "good_word_count = pipe['clf'].feature_count_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00pm</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>71.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bad   good\n",
       "word              \n",
       "00     30.0   32.0\n",
       "000     2.0    8.0\n",
       "00pm    0.0    7.0\n",
       "10     71.0  106.0\n",
       "10 15   1.0    5.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of words with their separate 1-star and 5-stars counts\n",
    "words = pd.DataFrame({'word' : words, 'bad' : bad_word_count, 'good' : good_word_count}).set_index('word')\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1 to the columns counts to avoid dividing by 0\n",
    "words.bad = words.bad+1\n",
    "words.good = words.good+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00pm</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 15</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bad      good\n",
       "word                     \n",
       "00     0.000635  0.000214\n",
       "000    0.000061  0.000058\n",
       "00pm   0.000020  0.000052\n",
       "10     0.001476  0.000695\n",
       "10 15  0.000041  0.000039"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the counts into frequencies\n",
    "words.bad = words.bad/words.bad.sum()\n",
    "words.good = words.good/words.good.sum()\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios\n",
    "words['bad_ratio'] = words.bad/words.good\n",
    "words['good_ratio'] = words.good/words.bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>bad_ratio</th>\n",
       "      <th>good_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fantastic</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.031403</td>\n",
       "      <td>31.843646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.038645</td>\n",
       "      <td>25.876263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorite</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.040671</td>\n",
       "      <td>24.587730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love place</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.042939</td>\n",
       "      <td>23.288636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one best</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>22.496506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one favorite</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.045086</td>\n",
       "      <td>22.179654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yum</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.055369</td>\n",
       "      <td>18.060575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfection</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.067150</td>\n",
       "      <td>14.892053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bianco</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.078901</td>\n",
       "      <td>12.674088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasty</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.078901</td>\n",
       "      <td>12.674088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasonably</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>12.357236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca wait</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>12.357236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gem</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>12.357236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasonably price</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>11.723531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.087668</td>\n",
       "      <td>11.406679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly recommend</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.089322</td>\n",
       "      <td>11.195444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delish</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.095638</td>\n",
       "      <td>10.456122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dentist</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.095638</td>\n",
       "      <td>10.456122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mozzarella</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.098626</td>\n",
       "      <td>10.139270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pastry</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.098626</td>\n",
       "      <td>10.139270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       bad      good  bad_ratio  good_ratio\n",
       "word                                                       \n",
       "fantastic         0.000041  0.001305   0.031403   31.843646\n",
       "perfect           0.000061  0.001591   0.038645   25.876263\n",
       "favorite          0.000102  0.002520   0.040671   24.587730\n",
       "love place        0.000041  0.000955   0.042939   23.288636\n",
       "one best          0.000020  0.000461   0.044451   22.496506\n",
       "one favorite      0.000020  0.000455   0.045086   22.179654\n",
       "yum               0.000020  0.000370   0.055369   18.060575\n",
       "perfection        0.000020  0.000305   0.067150   14.892053\n",
       "bianco            0.000020  0.000260   0.078901   12.674088\n",
       "pasty             0.000020  0.000260   0.078901   12.674088\n",
       "reasonably        0.000020  0.000253   0.080924   12.357236\n",
       "ca wait           0.000041  0.000507   0.080924   12.357236\n",
       "gem               0.000020  0.000253   0.080924   12.357236\n",
       "reasonably price  0.000020  0.000240   0.085299   11.723531\n",
       "candy             0.000020  0.000234   0.087668   11.406679\n",
       "highly recommend  0.000061  0.000688   0.089322   11.195444\n",
       "delish            0.000020  0.000214   0.095638   10.456122\n",
       "dentist           0.000020  0.000214   0.095638   10.456122\n",
       "mozzarella        0.000020  0.000208   0.098626   10.139270\n",
       "pastry            0.000020  0.000208   0.098626   10.139270"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sort_values(by='good_ratio', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>bad_ratio</th>\n",
       "      <th>good_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>horrible</th>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>35.618230</td>\n",
       "      <td>0.028076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never return</th>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>34.716502</td>\n",
       "      <td>0.028805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor service</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>31.560457</td>\n",
       "      <td>0.031685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service horrible</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>31.560457</td>\n",
       "      <td>0.031685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rude</th>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>28.855275</td>\n",
       "      <td>0.034656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never come</th>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>28.404411</td>\n",
       "      <td>0.035206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wo back</th>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>28.404411</td>\n",
       "      <td>0.035206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste money</th>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>28.404411</td>\n",
       "      <td>0.035206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ugh</th>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>26.826388</td>\n",
       "      <td>0.037277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unacceptable</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disaster</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inedible</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speak manager</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poorly</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hubster</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spill</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fedex</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filthy</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rudely</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horrible service</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>25.248365</td>\n",
       "      <td>0.039607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       bad      good  bad_ratio  good_ratio\n",
       "word                                                       \n",
       "horrible          0.001619  0.000045  35.618230    0.028076\n",
       "never return      0.000225  0.000006  34.716502    0.028805\n",
       "poor service      0.000205  0.000006  31.560457    0.031685\n",
       "service horrible  0.000205  0.000006  31.560457    0.031685\n",
       "rude              0.001312  0.000045  28.855275    0.034656\n",
       "never come        0.000184  0.000006  28.404411    0.035206\n",
       "wo back           0.000184  0.000006  28.404411    0.035206\n",
       "waste money       0.000184  0.000006  28.404411    0.035206\n",
       "ugh               0.000348  0.000013  26.826388    0.037277\n",
       "unacceptable      0.000164  0.000006  25.248365    0.039607\n",
       "disaster          0.000164  0.000006  25.248365    0.039607\n",
       "inedible          0.000164  0.000006  25.248365    0.039607\n",
       "speak manager     0.000164  0.000006  25.248365    0.039607\n",
       "poorly            0.000164  0.000006  25.248365    0.039607\n",
       "hubster           0.000164  0.000006  25.248365    0.039607\n",
       "spill             0.000164  0.000006  25.248365    0.039607\n",
       "fedex             0.000164  0.000006  25.248365    0.039607\n",
       "filthy            0.000164  0.000006  25.248365    0.039607\n",
       "rudely            0.000164  0.000006  25.248365    0.039607\n",
       "horrible service  0.000164  0.000006  25.248365    0.039607"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sort_values(by='bad_ratio', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I always have customer service problems with Fedex.  How the heck did they get to be so big with the crappy way they treat customers?  I use the Internet a lot for purchases and about 60% of the time, they screwed-up and then their customer service has major difficulty fixing the problems.  I have asked for shipments to be held at the closet shipping location to my office and they ship to my home.  Another time I asked for the same thing and they held it at the regional depot instead of where I asked that it be sent.  I can go on and on about the screw-ups I have experienced with Fedex.  \\n\\nBut this time they really pissed me off.  I get a harassing phone calls from a collection agency that says they have been contracted by Fedex to collect an overdue bill from me in the amount of $4000+.  I have asked them for details and all I get is harassment.  So I asked Fedex for help and customer service told me to contact Fedex Revenue Services, which I did.  I told them that I am getting harassing phone calls from XYZ collection agency and the person who keeps calling is named Chris.  I also tell them that the alleged amount owed is $4000+ and the account is supposedly in my name.  So I ask that they investigate and tell me if I indeed have an overdue account in my name for this amount.  (I know I do not have such an account!).  Fedex then tells me that they cannot help me because I have not provided enough information.  WTF?  How much more information can I provide?  Do they want my height and weight?  Maybe my blood type?  Maybe I have to provide the names and birth dates of my children too?\\n\\nThis is why I say that Fedex really blows!'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp[yelp.processed_text.str.contains('fedex')].iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOW this place is good!  SO good!  And not just yummy good, but intrinsically good.  Check out their amazing list of environmentally responsible business practices - http://essencebakery.com/essence_bakery_environmentally_friendly.shtml !  \n",
      "\n",
      "That, and it's cute.  And it tastes Yummy!  And they are SO nice!  SOOO nice!  And their deserts are just disgustingly cute and beautiful and freaking good.  I almost bought the mini box of 4 cupcakes for just $3.50.  Bite sized so not too bad, but if I bought them today they'd be gone before I got home. \n",
      "\n",
      "I got their grilled cheese w/ mozzarella, basil and tomato on grilled buttery brioche bread with a light and tasty green salad on the side.  It will be hard to not come here every week.  I love that all the drinks are refillable - and it's up to you to refill them.  Coffee, tea or soda, just walk on up and fill your glass.  And I love that it's all so fresh and local!  \n",
      "\n",
      "Important note - You order at the counter, they give you a number and bring out your food.  If you're paying w/ debit card, they will ask you if you want to include a tip.  That threw me.  I'd been in the place less than 2 minutes - how do I know if it's worth the tip - or how much of a tip it's worth??  Bring some cash or plan to deal w/ that question accordingly.\n"
     ]
    }
   ],
   "source": [
    "print(yelp[yelp.processed_text.str.contains('mozzarella')].iloc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
